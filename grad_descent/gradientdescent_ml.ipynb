{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent for Machine Learning\n",
    "\n",
    "To determine the best fit line using machine learning, different combinations of slope and intercept are tested. The combination with minimum error, meaning minimum distance of point from the line (squared to ensure the value is always positive), will be the line of best fit.\n",
    "\n",
    "Mean-squared Error:\n",
    "\n",
    "![Mean-squared error](mserror.png)\n",
    "\n",
    "This can be illustrated by the 3-axes diagram shown below:\n",
    "\n",
    "![Gradient Descent Illustration](gradientdescent.png)\n",
    "\n",
    "Separate the variables and plot them agains the mean-squared error function separately. The graph should look like a parabolic function with a minimum (show in picture below):\n",
    "\n",
    "![Gradient Descent Illustration 2](Gd2.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to perform gradient descent (regardless of loss function used)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Take the derivative of the Loss Function for each parameter in it. (take gradietn of loss function)\n",
    "\n",
    "E.g.\n",
    "\n",
    ">$\\sum$ squared residuals (loss function) = $\\sum(observed_{value} - predicted_{value})^2$\n",
    ">\n",
    ">$\\sum$ squared residuals (loss function) = $\\sum(observed_{value} - (intercept + slope*weight))^2$\n",
    ">\n",
    ">$\\frac{d}{d_{intercept}}$ $\\sum$ squared residuals (loss function)\n",
    ">\n",
    ">$\\frac{d}{d_{slope}}$ $\\sum$ squared residuals (loss function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Pick random values for the parameters.\n",
    "\n",
    "Pick random intercept and weight values.\n",
    "\n",
    ">$intercept = 0.5$\n",
    ">\n",
    ">$slope = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Plug the parameter values into the derivatives (ahem, the Gradient)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Calculate the step sizes:\n",
    "\n",
    ">***Step size*** = *Derived Parameter* x *Learning Rate*\n",
    "\n",
    "> Learning rate is arbitrary. (0.001-0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Calculate the New Parameters\n",
    "\n",
    ">***New Parameter*** = *Old Parameter* - *Step Size*\n",
    ">\n",
    ">Repeat Step 3-5 until **Step Size** is very small or reach *maximum* number of steps.\n",
    "\n",
    ">Step size is directly proportional to parameter which is the partial derivatives of original loss function (the slopes). So when step size approaches 0, the parameters are optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib widget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, y):\n",
    "    return np.sin(np.sqrt(x ** 2 + y ** 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Input data arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = np.linspace(-6, 6, 30)\n",
    "y = np.linspace(-6, 6, 30)\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = f(X,Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,10))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "ax.set_zlabel('z')\n",
    "ax.set_title('3D surface')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('DataScience')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "15c829b79a9c192d7731c00829287f4b3b1aa9f9a3890f935442b2f4d956bef3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
