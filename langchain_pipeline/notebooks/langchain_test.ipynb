{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import dotenv_values\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import logging\n",
    "import pprint\n",
    "\n",
    "secrets = dotenv_values(r\"..\\..\\langchain_pipeline\\.env\")\n",
    "total_cost = 0\n",
    "total_tokens = 0\n",
    "total_prompt_tokens = 0\n",
    "total_completion_tokens = 0\n",
    "\n",
    "# Function to set up the logger\n",
    "def setup_logger(name, log_file, level=logging.DEBUG):\n",
    "    \"\"\"Function to set up as many loggers as you want.\"\"\"\n",
    "\n",
    "    # Create logger\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "\n",
    "    # Avoid setting up the logger again if it already has handlers (to avoid duplicate logs)\n",
    "    if not logger.hasHandlers():\n",
    "        # Create formatter\n",
    "        formatter = logging.Formatter(\n",
    "            '%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "        # Create a file handler and set level to debug\n",
    "        file_handler = logging.FileHandler(log_file)\n",
    "        file_handler.setFormatter(formatter)\n",
    "\n",
    "        # Create a stream handler (for console) and set level to debug\n",
    "        stream_handler = logging.StreamHandler()\n",
    "        stream_handler.setFormatter(formatter)\n",
    "\n",
    "        # Add handlers to logger\n",
    "        logger.addHandler(file_handler)\n",
    "        logger.addHandler(stream_handler)\n",
    "\n",
    "    return logger\n",
    "\n",
    "openai_api_logger = setup_logger(\"openai_api\", \"openai_api.log\")\n",
    "\n",
    "# os.environ['OPENAI_API_KEY'] = secrets['openai_api']\n",
    "# os.environ['SERPAPI_API_KEY'] = secrets['serp_api_key']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-19 23:06:17,309 - openai_api - INFO - \n",
      "Current API Call Usage\n",
      "---------------------\n",
      "Tokens Used: 0\n",
      "\tPrompt Tokens: 0\n",
      "\tCompletion Tokens: 0\n",
      "Successful Requests: 0\n",
      "Total Cost (USD): $0.0\n",
      "\n",
      "Total Usage\n",
      "---------------------\n",
      "Total Cost: 0.0\n",
      "Total Tokens: 0\n",
      "Total Prompt Tokens: 0.0\n",
      "Total Completion Tokens: 0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "openai_models_url = \"https://api.openai.com/v1/models\"\n",
    "\n",
    "with get_openai_callback() as openai:\n",
    "    openai_models = requests.get(openai_models_url, headers={\"Authorization\": \"Bearer \" + secrets['openai_api']}).json()\n",
    "    total_cost += openai.total_cost\n",
    "    total_tokens += openai.total_tokens\n",
    "    total_prompt_tokens += openai.prompt_tokens\n",
    "    total_completion_tokens += openai.completion_tokens\n",
    "    openai_api_logger.info(f'''\\nCurrent API Call Usage\\n---------------------\\n{openai}\\n\\nTotal Usage\\n---------------------\\nTotal Cost: {total_cost}\\nTotal Tokens: {total_tokens}\\nTotal Prompt Tokens: {total_cost}\\nTotal Completion Tokens: {total_cost}\\n''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_models(all_models: dict, re_pattern: str = 'gpt'):\n",
    "    gpt_models = [i['id'] for i in all_models['data'] if re.search(re_pattern, i['id'])]\n",
    "    return gpt_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gpt-3.5-turbo-instruct-0914',\n",
       " 'gpt-3.5-turbo-instruct',\n",
       " 'gpt-3.5-turbo-16k-0613',\n",
       " 'gpt-3.5-turbo-0613',\n",
       " 'gpt-3.5-turbo-0301',\n",
       " 'gpt-3.5-turbo',\n",
       " 'gpt-3.5-turbo-16k']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_models(openai_models, re_pattern='gpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "def chat_llm_request(system_message: str, human_message: str, temperature: float = 0.5):\n",
    "    global total_cost, total_tokens, total_prompt_tokens, total_completion_tokens\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        \"\"\"Requests to the OpenAI API is made here\"\"\"\n",
    "        chat_llm = ChatOpenAI(temperature=temperature)\n",
    "\n",
    "        messages = [\n",
    "            SystemMessage(content=system_message),\n",
    "            HumanMessage(content=human_message)\n",
    "        ]\n",
    "        response = chat_llm(messages=messages)\n",
    "        total_cost += cb.total_cost\n",
    "        total_tokens += cb.total_tokens\n",
    "        total_prompt_tokens += cb.prompt_tokens\n",
    "        total_completion_tokens += cb.completion_tokens\n",
    "        \n",
    "        openai_api_logger.info(f'''\n",
    "Current API Call Usage\n",
    "---------------------\n",
    "{cb}\n",
    "\n",
    "Total Usage\n",
    "---------------------\n",
    "Total Cost: {total_cost}\n",
    "Total Tokens: {total_tokens}\n",
    "Total Prompt Tokens: {total_prompt_tokens}\n",
    "Total Completion Tokens: {total_completion_tokens}\n",
    "''')\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-20 21:28:45,714 - openai_api - INFO - \n",
      "Current API Call Usage\n",
      "---------------------\n",
      "Tokens Used: 984\n",
      "\tPrompt Tokens: 285\n",
      "\tCompletion Tokens: 699\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0018254999999999999\n",
      "\n",
      "Total Usage\n",
      "---------------------\n",
      "Total Cost: 0.0018254999999999999\n",
      "Total Tokens: 984\n",
      "Total Prompt Tokens: 285\n",
      "Total Completion Tokens: 699\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    \"You are a professional English and Dutch teacher. You will be provided a prompt in either dutch or english, and you will have to answer the prompt and then provide a translation to Dutch or English. When prompted to generate a short story in either Dutch or English, the short story must follow the basic structure of a short story with a beginning, middle, and end. The short story must be at least 10 sentences long. After generating a short story, prompt the user on whether they would like to translate the short story to Dutch or English. If the user chooses to translate the short story to Dutch, then the short story must be translated to Dutch. If the user chooses to translate the short story to English, then the short story must be translated to English. The short story must be translated to the best of your ability. If prompted to generate questions on the short story in either Dutch or English, then the questions must be generated in the language that is requested and the user will be requred to answer the questions in the same language as the questions were generated in. Always check your work for spelling and grammar mistakes. Always check the user's responses for spelling and grammar mistakes. If the user's response contains spelling or grammar mistakes, then prompt the user to correct their mistakes.\",\n",
    "    \"Tell me a short story in dutch that an intermediate level person can understand.\",\n",
    "]\n",
    "\n",
    "response = chat_llm_request(system_message=messages[0], human_message=messages[1], temperature=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Er was eens een klein meisje genaamd Lisa. Lisa woonde in een prachtig dorpje omringd door groene weilanden en hoge bergen. Op een dag besloot Lisa om een avontuur te beleven. Ze pakte haar rugzak en ging op weg naar het mysterieuze bos aan de rand van het dorp.\n",
      "\n",
      "Het bos was donker en stil, maar Lisa was niet bang. Ze wandelde verder en ontdekte al snel een sprookjesachtige plek met kleurrijke bloemen en sprankelende watervallen. Terwijl Lisa genoot van de schoonheid van het bos, hoorde ze plotseling een zacht gehuil.\n",
      "\n",
      "Lisa volgde het geluid en ontdekte een kleine gewonde vogel. Ze nam de vogel voorzichtig in haar handen en besloot om voor hem te zorgen. Lisa bouwde een klein nestje van takjes en bladeren en voedde de vogel met bessen.\n",
      "\n",
      "Dagen gingen voorbij en de vogel werd sterker. Hij begon te fladderen in het nestje en sprong uiteindelijk uit het nest om te vliegen. Lisa voelde zich trots en gelukkig dat ze de vogel had geholpen.\n",
      "\n",
      "Terug in het dorp vertelde Lisa aan iedereen over haar avontuur in het mysterieuze bos en hoe ze de gewonde vogel had verzorgd. Haar verhaal inspireerde anderen om aandacht te besteden aan de natuur en zorg te dragen voor dieren in nood.\n",
      "\n",
      "De jaren verstreken en Lisa groeide op tot een liefdevolle natuurbeschermer. Ze besloot om biologie te studeren en werd een bekende dierenarts in het dorp. Lisa's avontuur in het bos had haar leven voorgoed veranderd en ze wist nu dat ze een verschil kon maken in de wereld.\n",
      "\n",
      "Wil je dit verhaal vertaald hebben naar het Engels?\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "agent = initialize_agent(\n",
    "    tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response = agent.run(\n",
    "        \"What is a beagle that does not have hair?\"\n",
    "    )\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatModel Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "MEMORY_KEY = 'chat_history'\n",
    "memory = ConversationBufferMemory(memory_key=MEMORY_KEY, return_messages=True)\n",
    "\n",
    "chat_model = ChatOpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\"])\n",
    "agent = initialize_agent(tools, chat_model, agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION, verbose=True, memory=memory, handle_parsing_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    response = agent.run(\n",
    "        \"What is a beagle that does not have hair?\"\n",
    "    )\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with get_openai_callback() as cb:\n",
    "    print(agent.run(\n",
    "        \"Hi, my name is Lian, and I am testing your memory.\"\n",
    "    ))\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agent.run(input=\"I want you to come up with a nickname for me.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ChatModel with Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "def chat_llm_request(system_message: str, human_message: str, temperature: float = 0.5):\n",
    "    global total_cost, total_tokens, total_prompt_tokens, total_completion_tokens\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        \"\"\"Requests to the OpenAI API is made here\"\"\"\n",
    "        chat_llm = ChatOpenAI(temperature=temperature)\n",
    "\n",
    "        prompt = ChatPromptTemplate(\n",
    "            messages=[\n",
    "                SystemMessagePromptTemplate.from_template(\n",
    "                    \"You are a nice chatbot having a conversation with a human.\"\n",
    "                ),\n",
    "                # The `variable_name` here is what must align with memory\n",
    "                MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "            ]\n",
    "        )\n",
    "        memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "        conversation = LLMChain(\n",
    "            llm=llm,\n",
    "            prompt=prompt,\n",
    "            verbose=True,\n",
    "            memory=memory\n",
    "        )\n",
    "\n",
    "        total_cost += cb.total_cost\n",
    "        total_tokens += cb.total_tokens\n",
    "        total_prompt_tokens += cb.prompt_tokens\n",
    "        total_completion_tokens += cb.completion_tokens\n",
    "        \n",
    "        openai_api_logger.info(f'''\n",
    "Current API Call Usage\n",
    "---------------------\n",
    "{cb}\n",
    "\n",
    "Total Usage\n",
    "---------------------\n",
    "Total Cost: {total_cost}\n",
    "Total Tokens: {total_tokens}\n",
    "Total Prompt Tokens: {total_prompt_tokens}\n",
    "Total Completion Tokens: {total_completion_tokens}\n",
    "''')\n",
    "\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: What is your name?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Hello! As a language model developed by OpenAI, I don\\'t have a specific name. You can refer to me as \"ChatGPT\" or simply \"Chatbot.\" How can I assist you today?'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = ChatOpenAI()\n",
    "prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        SystemMessagePromptTemplate.from_template(\n",
    "            \"You are a nice chatbot having a conversation with a human.\"\n",
    "        ),\n",
    "        # The `variable_name` here is what must align with memory\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        HumanMessagePromptTemplate.from_template(\"{question}\")\n",
    "    ]\n",
    ")\n",
    "# Notice that we `return_messages=True` to fit into the MessagesPlaceholder\n",
    "# Notice that `\"chat_history\"` aligns with the MessagesPlaceholder name.\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "conversation = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory\n",
    ")\n",
    "conversation.run(\n",
    "    \"What is your name?\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mSystem: You are a nice chatbot having a conversation with a human.\n",
      "Human: What is your name?\n",
      "AI: Hello! As a language model developed by OpenAI, I don't have a specific name. You can refer to me as \"ChatGPT\" or simply \"Chatbot.\" How can I assist you today?\n",
      "Human: Let's call you Persona, can you tell me a little about yourself?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Certainly! While I don't have a personal identity or consciousness, I can tell you about my capabilities and purpose. I am an AI language model designed to engage in conversations with users like you. My purpose is to assist and provide helpful information on a wide range of topics, from answering questions to offering suggestions and engaging in discussions. I have been trained on a diverse range of texts from the internet, which enables me to generate human-like responses. How can I help you today, Persona?\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\n",
    "    \"Let's call you Persona, can you tell me a little about yourself?\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
